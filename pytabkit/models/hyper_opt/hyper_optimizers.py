import time
from pathlib import Path
from typing import Callable, Tuple, Any, Dict, Union

import numpy as np

from pytabkit.models import utils
from pytabkit.models.training.logging import Logger


class FunctionEvaluationTracker:
    """
    Helper class to keep track of where the function to be optimized is evaluated and what are the best parameters
    """
    def __init__(self, f: Callable[[dict], Tuple[float, Any]], n_steps: int, opt_desc: str, logger: Logger):
        self.f = f
        self.n_steps = n_steps
        self.opt_desc = opt_desc
        self.logger = logger
        self.best_params = None
        self.best_result = None
        self.n_calls = 0

    def __call__(self, params: dict) -> Tuple[float, Any]:
        # params = utils.join_dicts(params, self.fixed_params)
        start_time = time.time()
        result = self.f(params)
        if np.isnan(result[0]):
            result = (np.inf, result[1])
        eval_time = time.time() - start_time
        if self.best_result is None or (result[0] <= self.best_result[0]):
            # print(f'new best result')
            self.best_params = params
            self.best_result = result
        self.n_calls += 1
        self.logger.log(-1, f'Hyperopt step {self.n_calls}/{self.n_steps} on {self.opt_desc} took {eval_time:g} s')

        # don't return the second part of result as HPO libraries might store all of them, causing RAM problems
        return result[0], None

    def get_best_params_and_result(self) -> Tuple[Dict, Tuple[float, Any]]:
        return self.best_params, self.best_result


class HyperOptimizer:
    def __init__(self, n_hyperopt_steps: int):
        self.n_hyperopt_steps = n_hyperopt_steps

    def _optimize_impl(self, f: Callable[[dict], Tuple[float, Any]], seed: int) -> None:
        # override this in subclasses
        raise NotImplementedError()

    def optimize(self, f: Callable[[dict], Tuple[float, Any]], seed: int, opt_desc: str, logger: Logger) \
            -> Tuple[Dict, Any]:
        """
        :param f: Function to minimize. It should take a dict of parameters
        and return a tuple containing the validation loss and additional information about the run
        (additional information could for example be the early stopping epoch found in this particular run,
        for example {'n_estimators': best_n_estimators})
        :param seed: Random seed for optimization
        :param opt_desc: name of the optimized algorithm / optimization problem
         (used for printing optimization intermediate state)
        :param logger: Logger used for printing information
        :return: Returns a tuple containing a dictionary with the optimal parameters
        and the additional info generated by the function at the optimal parameters
        """
        # todo: could also add verbosity level
        # todo: may need to be able to treat failures, hence make the tuple optional?
        # todo: could allow to pass the iteration number to the function
        tracker = FunctionEvaluationTracker(f, n_steps=self.n_hyperopt_steps, opt_desc=opt_desc, logger=logger)
        self._optimize_impl(tracker, seed=seed)
        best_params, best_result = tracker.get_best_params_and_result()
        return best_params, best_result[1]

    def get_n_hyperopt_steps(self) -> int:
        return self.n_hyperopt_steps


# todo: have one class that does performance tracking of all intermediate steps  (or do that in HyperoptAlgInterface?)
# and maybe also do logging separately?
# maybe have wrapper function / callable class that tracks it?
# Then implement something like _optimize() that gets the wrapped function?


class ConstantHyperOptimizer(HyperOptimizer):
    def __init__(self, params: dict):
        super().__init__(n_hyperopt_steps=1)
        self.params = params

    def _optimize_impl(self, f: Callable[[dict], Tuple[float, Any]], seed: int) -> None:
        f(self.params)


def f_unpack_dict(dct):
    """
    Unpacks all sub-dictionaries in given dictionary recursively.
    There should be no duplicated keys across all nested
    subdictionaries, or some instances will be lost without warning

    Source: https://www.kaggle.com/fanvacoolt/tutorial-on-hyperopt

    Parameters:
    ----------------
    dct : dictionary to unpack

    Returns:
    ----------------
    : unpacked dictionary
    """

    res = {}
    for (k, v) in dct.items():
        if isinstance(v, dict):
            res = {**res, **f_unpack_dict(v)}
        else:
            res[k] = v

    return res


class HyperoptOptimizer(HyperOptimizer):
    class HyperoptFuncWrapper:
        def __init__(self, f: Callable[[dict], Tuple[float, Any]], fixed_params: dict):
            self.f = f
            self.fixed_params = fixed_params

        def __call__(self, params: dict):
            params = f_unpack_dict(params)  # for nested/conditional params
            from hyperopt import STATUS_FAIL, STATUS_OK
            params = utils.join_dicts(params, self.fixed_params)
            loss, additional_info = self.f(params)
            return {'loss': loss, 'additional_info': additional_info,
                    'status': STATUS_FAIL if np.isnan(loss) else STATUS_OK,
                    'params': params.copy()}

    def __init__(self, space, fixed_params, n_hyperopt_steps: int = 50, **config):
        super().__init__(n_hyperopt_steps=n_hyperopt_steps)
        self.space = space
        self.fixed_params = fixed_params
        self.config = config

    def _optimize_impl(self, f: Callable[[dict], Tuple[float, Any]], seed: int) -> None:
        import hyperopt
        trials = hyperopt.Trials()  # todo: could serialize the trials object for restarting
        algo_name = self.config.get('hyperopt_algo', 'tpe')
        if algo_name == 'tpe':
            algo = hyperopt.tpe.suggest
        elif algo_name == 'atpe':
            # atpe seems to be not deterministic even when setting the seed...
            raise ValueError('atpe for hyperopt is not implemented since it is not deterministic and can throw errors')
            # print(f'Using atpe', flush=True)
            # algo = hyperopt.atpe.suggest
        elif algo_name == 'rand':
            print(f'Using rand', flush=True)
            algo = hyperopt.rand.suggest
        else:
            raise ValueError(f'Unknown hyperopt_algo name "{algo_name}"')
        fn = HyperoptOptimizer.HyperoptFuncWrapper(f, self.fixed_params)
        _ = hyperopt.fmin(fn=fn,
                          space=self.space, algo=algo, max_evals=self.n_hyperopt_steps, trials=trials,
                          rstate=np.random.default_rng(seed=seed), verbose=False, show_progressbar=False)


class SMACOptimizer(HyperOptimizer):
    class SMACFuncWrapper:
        def __init__(self, f: Callable[[dict], Tuple[float, Any]], fixed_params: Dict[str, Any]):
            self.f = f
            self.fixed_params = fixed_params

        def __call__(self, params, seed: int = 0):
            # params should be of type ConfigSpace.Configuration
            params = params.get_dictionary()
            params = utils.join_dicts(params, self.fixed_params)
            loss, additional_info = self.f(params)
            return np.inf if np.isnan(loss) else loss

    def __init__(self, space, fixed_params: Dict[str, Any], n_hyperopt_steps: int = 50,
                 tmp_folder: Union[str, Path] = 'smac3_output', **config):
        super().__init__(n_hyperopt_steps=n_hyperopt_steps)
        self.space = space
        self.n_hyperopt_steps = n_hyperopt_steps
        self.fixed_params = fixed_params
        self.config = config
        self.tmp_folder = tmp_folder

    def _optimize_impl(self, f: Callable[[dict], Tuple[float, Any]], seed: int) -> None:
        use_gp = self.config.get('smac_surrogate', 'RF') == 'GP'
        fn = SMACOptimizer.SMACFuncWrapper(f, self.fixed_params)

        import smac
        scenario = smac.Scenario(self.space, deterministic=True, n_trials=self.n_hyperopt_steps,
                                 seed=seed, use_default_config=True, output_directory=self.tmp_folder)

        max_ratio = 0.25
        n_configs_per_hyperparameter = 8 if use_gp else 10
        if 'n_initial_design' in self.config:
            max_ratio = self.config['n_initial_design'] / self.n_hyperopt_steps
            n_configs_per_hyperparameter = self.config['n_initial_design']

        from smac.initial_design import SobolInitialDesign
        initial_design = SobolInitialDesign(
            scenario=scenario,
            n_configs=None,
            n_configs_per_hyperparameter=n_configs_per_hyperparameter,
            max_ratio=max_ratio,
            additional_configs=[],
        )

        # Now we use SMAC to find the best hyperparameters
        if use_gp:
            print(f'Using SMAC with GP surrogate')
            facade = smac.BlackBoxFacade(
                scenario=scenario,
                target_function=fn.__call__,
                overwrite=True,
                logging_level=False,
                initial_design=initial_design
            )
        else:
            facade = smac.HyperparameterOptimizationFacade(
                scenario,
                fn.__call__,  # We pass the target function here
                overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data
                logging_level=False,  # no logging
                initial_design=initial_design,
            )
        facade.optimize()
